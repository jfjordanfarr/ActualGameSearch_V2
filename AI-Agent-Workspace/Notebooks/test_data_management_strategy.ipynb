{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d39c1c",
   "metadata": {},
   "source": [
    "# Test Data Management Strategy for ActualGameSearch V2\n",
    "\n",
    "This notebook documents our approach to handling test data in the ActualGameSearch V2 pipeline, ensuring robust testing without committing sensitive or large datasets to version control.\n",
    "\n",
    "## Background\n",
    "\n",
    "The ActualGameSearch V2 project processes large Steam game datasets, price information, and user reviews. While this data is essential for our ETL pipeline, we cannot and should not commit it to version control due to:\n",
    "\n",
    "- **Size constraints**: Data files can be hundreds of MB to GB\n",
    "- **Privacy concerns**: User review data may contain sensitive information\n",
    "- **API rate limits**: Steam API data should not be redistributed\n",
    "- **Repository bloat**: Large binary files make git operations slow\n",
    "\n",
    "This notebook outlines our solution: **fixture-based testing with synthetic data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4da1b",
   "metadata": {},
   "source": [
    "## Section 1: Understanding Test Data Requirements\n",
    "\n",
    "Let's analyze what types of data our tests need and categorize them by sensitivity, size, and generation complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95440dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Define our test data categories\n",
    "test_data_categories = {\n",
    "    \"steam_app_data\": {\n",
    "        \"sensitivity\": \"Low\",  # Public Steam data\n",
    "        \"size\": \"Large\",  # 100k+ apps\n",
    "        \"generation_complexity\": \"Medium\",  # Structured but varied\n",
    "        \"examples\": [\"steam_appid\", \"name\", \"type\", \"is_free\", \"detailed_description\"]\n",
    "    },\n",
    "    \"price_data\": {\n",
    "        \"sensitivity\": \"Low\",  # Historical pricing, publicly available\n",
    "        \"size\": \"Medium\",  # Price points per app\n",
    "        \"generation_complexity\": \"Low\",  # Simple numeric ranges\n",
    "        \"examples\": [\"min_price\", \"max_price\", \"sample_count\"]\n",
    "    },\n",
    "    \"review_data\": {\n",
    "        \"sensitivity\": \"Medium\",  # User-generated content\n",
    "        \"size\": \"Very Large\",  # Millions of reviews\n",
    "        \"generation_complexity\": \"High\",  # Natural language, sentiment\n",
    "        \"examples\": [\"review_text\", \"helpful_votes\", \"sentiment_score\"]\n",
    "    },\n",
    "    \"embeddings\": {\n",
    "        \"sensitivity\": \"Low\",  # Derived vectors\n",
    "        \"size\": \"Large\",  # High-dimensional vectors\n",
    "        \"generation_complexity\": \"High\",  # Requires ML models\n",
    "        \"examples\": [\"description_embedding\", \"review_embedding\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display our analysis\n",
    "for category, details in test_data_categories.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for key, value in details.items():\n",
    "        if key != \"examples\":\n",
    "            print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {', '.join(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2fe1b",
   "metadata": {},
   "source": [
    "## Section 2: Setting Up Local Test Data Generation\n",
    "\n",
    "Create scripts to generate test data locally using libraries like Faker and custom generators that match our data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install faker if not already available\n",
    "# pip install faker\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def generate_steam_app_data(num_apps: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate synthetic Steam app data for testing.\"\"\"\n",
    "    \n",
    "    game_types = ['game', 'dlc', 'demo', 'software']\n",
    "    \n",
    "    apps = []\n",
    "    for i in range(num_apps):\n",
    "        app_id = fake.random_int(min=1000, max=999999)\n",
    "        app_type = random.choice(game_types)\n",
    "        \n",
    "        # Generate realistic game names\n",
    "        if app_type == 'dlc':\n",
    "            name = f\"{fake.catch_phrase()} - {fake.word().title()} DLC\"\n",
    "        elif app_type == 'demo':\n",
    "            name = f\"{fake.catch_phrase()} Demo\"\n",
    "        else:\n",
    "            name = fake.catch_phrase()\n",
    "        \n",
    "        apps.append({\n",
    "            'steam_appid': app_id,\n",
    "            'name': name,\n",
    "            'type': app_type,\n",
    "            'is_free': random.choice([True, False]) if app_type != 'demo' else True,\n",
    "            'detailed_description': fake.text(max_nb_chars=500),\n",
    "            'short_description': fake.sentence(nb_words=10),\n",
    "            'header_image': f\"https://steamcdn-a.akamaihd.net/steam/apps/{app_id}/header.jpg\",\n",
    "            'release_date': fake.date_between(start_date='-10y', end_date='today').isoformat()\n",
    "        })\n",
    "    \n",
    "    return apps\n",
    "\n",
    "# Generate sample data\n",
    "sample_apps = generate_steam_app_data(5)\n",
    "print(\"Sample generated app data:\")\n",
    "for app in sample_apps[:2]:  # Show first 2\n",
    "    print(f\"\\nApp ID: {app['steam_appid']}\")\n",
    "    print(f\"Name: {app['name']}\")\n",
    "    print(f\"Type: {app['type']}\")\n",
    "    print(f\"Free: {app['is_free']}\")\n",
    "    print(f\"Description: {app['short_description'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_price_data(app_ids: List[int]) -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"Generate synthetic price data for given app IDs.\"\"\"\n",
    "    \n",
    "    price_data = {}\n",
    "    \n",
    "    for app_id in app_ids:\n",
    "        # Generate realistic price ranges\n",
    "        base_price = round(random.uniform(0.99, 59.99), 2)\n",
    "        \n",
    "        # Some games go on sale\n",
    "        if random.random() < 0.7:  # 70% chance of sales\n",
    "            min_price = round(base_price * random.uniform(0.1, 0.8), 2)\n",
    "        else:\n",
    "            min_price = base_price\n",
    "        \n",
    "        # Free games\n",
    "        if random.random() < 0.15:  # 15% free games\n",
    "            min_price = 0.0\n",
    "            base_price = 0.0\n",
    "        \n",
    "        price_data[app_id] = {\n",
    "            'min': min_price,\n",
    "            'max': base_price,\n",
    "            'count': random.randint(1, 100)  # Number of price samples\n",
    "        }\n",
    "    \n",
    "    return price_data\n",
    "\n",
    "# Generate price data for our sample apps\n",
    "app_ids = [app['steam_appid'] for app in sample_apps]\n",
    "sample_prices = generate_price_data(app_ids)\n",
    "\n",
    "print(\"Sample generated price data:\")\n",
    "for app_id, price_info in list(sample_prices.items())[:3]:\n",
    "    print(f\"App {app_id}: ${price_info['min']:.2f} - ${price_info['max']:.2f} ({price_info['count']} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85010fb6",
   "metadata": {},
   "source": [
    "## Section 3: Creating Mock Data Factories\n",
    "\n",
    "Build reusable data factories that can create consistent, reproducible test data on-demand for different test scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataFactory:\n",
    "    \"\"\"Factory for generating consistent test data across different test scenarios.\"\"\"\n",
    "    \n",
    "    def __init__(self, seed: int = 42):\n",
    "        \"\"\"Initialize with a seed for reproducible data generation.\"\"\"\n",
    "        self.fake = Faker()\n",
    "        Faker.seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def create_minimal_app_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"Create minimal app dataset for basic ETL testing.\"\"\"\n",
    "        apps = [\n",
    "            {\n",
    "                'steam_appid': 12345,\n",
    "                'name': 'Test Game 1',\n",
    "                'type': 'game',\n",
    "                'is_free': False,\n",
    "                'detailed_description': 'A detailed description of test game 1',\n",
    "                'short_description': 'Short desc 1',\n",
    "                'header_image': 'http://example.com/image1.jpg'\n",
    "            },\n",
    "            {\n",
    "                'steam_appid': 67890,\n",
    "                'name': 'Test Game 2',\n",
    "                'type': 'game',\n",
    "                'is_free': True,\n",
    "                'detailed_description': 'A detailed description of test game 2',\n",
    "                'short_description': 'Short desc 2',\n",
    "                'header_image': 'http://example.com/image2.jpg'\n",
    "            },\n",
    "            {\n",
    "                'steam_appid': 11111,\n",
    "                'name': 'Test DLC',\n",
    "                'type': 'dlc',\n",
    "                'is_free': False,\n",
    "                'detailed_description': 'A detailed description of test DLC',\n",
    "                'short_description': 'Short desc DLC',\n",
    "                'header_image': 'http://example.com/image3.jpg'\n",
    "            }\n",
    "        ]\n",
    "        return pd.DataFrame(apps)\n",
    "    \n",
    "    def create_price_dataset(self, app_ids: List[int]) -> Dict[int, Dict[str, Any]]:\n",
    "        \"\"\"Create corresponding price data for given app IDs.\"\"\"\n",
    "        # Fixed price data for reproducible tests\n",
    "        price_map = {\n",
    "            12345: {'min': 9.99, 'max': 19.99, 'count': 25},\n",
    "            67890: {'min': 0.0, 'max': 0.0, 'count': 1},\n",
    "            11111: {'min': 4.99, 'max': 9.99, 'count': 10}\n",
    "        }\n",
    "        \n",
    "        return {app_id: price_map.get(app_id, {'min': 0.0, 'max': 0.0, 'count': 0}) \n",
    "                for app_id in app_ids}\n",
    "    \n",
    "    def create_large_dataset(self, num_apps: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Create larger dataset for performance testing.\"\"\"\n",
    "        apps = generate_steam_app_data(num_apps)\n",
    "        return pd.DataFrame(apps)\n",
    "    \n",
    "    def save_fixtures(self, output_dir: Path):\n",
    "        \"\"\"Save test fixtures to files for use in tests.\"\"\"\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Save minimal app data\n",
    "        minimal_apps = self.create_minimal_app_dataset()\n",
    "        minimal_apps.to_csv(output_dir / 'sample_expanded_apps.csv', index=False)\n",
    "        \n",
    "        # Save corresponding price data\n",
    "        app_ids = minimal_apps['steam_appid'].tolist()\n",
    "        price_data = self.create_price_dataset(app_ids)\n",
    "        \n",
    "        with open(output_dir / 'sample_price_minmax.json', 'w') as f:\n",
    "            json.dump({str(k): v for k, v in price_data.items()}, f, indent=2)\n",
    "        \n",
    "        print(f\"Fixtures saved to {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "factory = TestDataFactory()\n",
    "test_apps = factory.create_minimal_app_dataset()\n",
    "print(\"\\nMinimal test dataset:\")\n",
    "print(test_apps[['steam_appid', 'name', 'type', 'is_free']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931478b",
   "metadata": {},
   "source": [
    "## Section 4: Environment-Based Data Configuration\n",
    "\n",
    "Implement configuration systems that use different data sources for local development, CI/CD, and production testing environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ce1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "class Environment(Enum):\n",
    "    LOCAL = \"local\"\n",
    "    CI = \"ci\"\n",
    "    STAGING = \"staging\"\n",
    "    PRODUCTION = \"production\"\n",
    "\n",
    "@dataclass\n",
    "class TestDataConfig:\n",
    "    \"\"\"Configuration for test data based on environment.\"\"\"\n",
    "    environment: Environment\n",
    "    use_real_data: bool\n",
    "    data_dir: Path\n",
    "    fixtures_dir: Path\n",
    "    max_test_records: Optional[int] = None\n",
    "    cache_enabled: bool = True\n",
    "    \n",
    "    @classmethod\n",
    "    def from_environment(cls) -> 'TestDataConfig':\n",
    "        \"\"\"Create config based on current environment variables.\"\"\"\n",
    "        env_name = os.getenv('TEST_ENV', 'local').lower()\n",
    "        environment = Environment(env_name)\n",
    "        \n",
    "        # Base paths\n",
    "        base_dir = Path(os.getenv('PROJECT_ROOT', '.'))\n",
    "        data_dir = base_dir / 'pipeline' / 'data'\n",
    "        fixtures_dir = base_dir / 'pipeline' / 'tests' / 'fixtures'\n",
    "        \n",
    "        if environment == Environment.LOCAL:\n",
    "            return cls(\n",
    "                environment=environment,\n",
    "                use_real_data=os.getenv('USE_REAL_DATA', 'false').lower() == 'true',\n",
    "                data_dir=data_dir,\n",
    "                fixtures_dir=fixtures_dir,\n",
    "                max_test_records=None,\n",
    "                cache_enabled=True\n",
    "            )\n",
    "        \n",
    "        elif environment == Environment.CI:\n",
    "            return cls(\n",
    "                environment=environment,\n",
    "                use_real_data=False,  # Never use real data in CI\n",
    "                data_dir=data_dir,\n",
    "                fixtures_dir=fixtures_dir,\n",
    "                max_test_records=100,  # Limit records for speed\n",
    "                cache_enabled=False\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # Staging/Production - minimal fixtures only\n",
    "            return cls(\n",
    "                environment=environment,\n",
    "                use_real_data=False,\n",
    "                data_dir=data_dir,\n",
    "                fixtures_dir=fixtures_dir,\n",
    "                max_test_records=10,\n",
    "                cache_enabled=True\n",
    "            )\n",
    "\n",
    "class TestDataProvider:\n",
    "    \"\"\"Provides test data based on environment configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[TestDataConfig] = None):\n",
    "        self.config = config or TestDataConfig.from_environment()\n",
    "        self.factory = TestDataFactory()\n",
    "    \n",
    "    def get_app_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Get app data appropriate for current environment.\"\"\"\n",
    "        if self.config.use_real_data and self._real_data_available():\n",
    "            return self._load_real_app_data()\n",
    "        else:\n",
    "            return self._load_fixture_app_data()\n",
    "    \n",
    "    def get_price_data(self, app_ids: List[int]) -> Dict[int, Dict[str, Any]]:\n",
    "        \"\"\"Get price data appropriate for current environment.\"\"\"\n",
    "        if self.config.use_real_data and self._real_data_available():\n",
    "            return self._load_real_price_data(app_ids)\n",
    "        else:\n",
    "            return self.factory.create_price_dataset(app_ids)\n",
    "    \n",
    "    def _real_data_available(self) -> bool:\n",
    "        \"\"\"Check if real data files exist.\"\"\"\n",
    "        required_files = [\n",
    "            self.config.data_dir / 'expanded_sampled_apps.csv',\n",
    "            self.config.data_dir / 'price_minmax.json'\n",
    "        ]\n",
    "        return all(f.exists() for f in required_files)\n",
    "    \n",
    "    def _load_real_app_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load real app data with optional record limiting.\"\"\"\n",
    "        df = pd.read_csv(self.config.data_dir / 'expanded_sampled_apps.csv')\n",
    "        if self.config.max_test_records:\n",
    "            df = df.head(self.config.max_test_records)\n",
    "        return df\n",
    "    \n",
    "    def _load_fixture_app_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load fixture app data.\"\"\"\n",
    "        fixture_file = self.config.fixtures_dir / 'sample_expanded_apps.csv'\n",
    "        if fixture_file.exists():\n",
    "            return pd.read_csv(fixture_file)\n",
    "        else:\n",
    "            # Generate on-the-fly if fixture doesn't exist\n",
    "            return self.factory.create_minimal_app_dataset()\n",
    "    \n",
    "    def _load_real_price_data(self, app_ids: List[int]) -> Dict[int, Dict[str, Any]]:\n",
    "        \"\"\"Load real price data.\"\"\"\n",
    "        with open(self.config.data_dir / 'price_minmax.json', 'r') as f:\n",
    "            all_prices = json.load(f)\n",
    "        \n",
    "        # Filter to requested app_ids and convert keys to int\n",
    "        return {app_id: all_prices.get(str(app_id), {'min': 0.0, 'max': 0.0, 'count': 0}) \n",
    "                for app_id in app_ids}\n",
    "\n",
    "# Example usage\n",
    "config = TestDataConfig.from_environment()\n",
    "print(f\"Current environment: {config.environment.value}\")\n",
    "print(f\"Use real data: {config.use_real_data}\")\n",
    "print(f\"Max test records: {config.max_test_records}\")\n",
    "print(f\"Fixtures directory: {config.fixtures_dir}\")\n",
    "\n",
    "provider = TestDataProvider(config)\n",
    "test_data = provider.get_app_data()\n",
    "print(f\"\\nLoaded {len(test_data)} test records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14330c",
   "metadata": {},
   "source": [
    "## Section 5: Test Data Fixtures and Snapshots\n",
    "\n",
    "Create small, representative data fixtures that can be committed to version control and larger datasets that are generated or downloaded as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c51a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine our current fixture structure\n",
    "workspace_root = Path.cwd()\n",
    "fixtures_dir = workspace_root / 'pipeline' / 'tests' / 'fixtures'\n",
    "\n",
    "print(f\"Fixtures directory: {fixtures_dir}\")\n",
    "print(f\"Exists: {fixtures_dir.exists()}\")\n",
    "\n",
    "if fixtures_dir.exists():\n",
    "    print(\"\\nCurrent fixture files:\")\n",
    "    for file in fixtures_dir.iterdir():\n",
    "        if file.is_file():\n",
    "            size_kb = file.stat().st_size / 1024\n",
    "            print(f\"  {file.name}: {size_kb:.1f} KB\")\n",
    "            \n",
    "            # Show content preview for small files\n",
    "            if size_kb < 5 and file.suffix in ['.csv', '.json']:\n",
    "                print(f\"    Preview: {file.read_text()[:100]}...\")\n",
    "\n",
    "# Guidelines for fixture design\n",
    "fixture_guidelines = {\n",
    "    \"Size limits\": {\n",
    "        \"Individual files\": \"< 10 KB each\",\n",
    "        \"Total fixtures\": \"< 100 KB total\",\n",
    "        \"Records per file\": \"< 50 records\"\n",
    "    },\n",
    "    \"Content guidelines\": {\n",
    "        \"Representative data\": \"Cover edge cases and common scenarios\",\n",
    "        \"Anonymized\": \"No real user data or sensitive information\",\n",
    "        \"Deterministic\": \"Same data every time for reproducible tests\",\n",
    "        \"Minimal but complete\": \"Just enough to test all code paths\"\n",
    "    },\n",
    "    \"File formats\": {\n",
    "        \"CSV\": \"For tabular data (apps, reviews)\",\n",
    "        \"JSON\": \"For structured data (price mappings, config)\",\n",
    "        \"Text\": \"For samples of descriptions, reviews\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n=== FIXTURE DESIGN GUIDELINES ===\")\n",
    "for category, rules in fixture_guidelines.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for rule, description in rules.items():\n",
    "        print(f\"  • {rule}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_fixtures():\n",
    "    \"\"\"Create a comprehensive set of test fixtures.\"\"\"\n",
    "    \n",
    "    factory = TestDataFactory(seed=42)  # Deterministic\n",
    "    \n",
    "    # Ensure fixtures directory exists\n",
    "    fixtures_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # 1. Minimal app dataset (already created)\n",
    "    minimal_apps = factory.create_minimal_app_dataset()\n",
    "    minimal_apps.to_csv(fixtures_dir / 'sample_expanded_apps.csv', index=False)\n",
    "    \n",
    "    # 2. Edge case apps (free games, DLC, different types)\n",
    "    edge_cases = pd.DataFrame([\n",
    "        {\n",
    "            'steam_appid': 99999,\n",
    "            'name': 'Free Game',\n",
    "            'type': 'game',\n",
    "            'is_free': True,\n",
    "            'detailed_description': '',  # Empty description edge case\n",
    "            'short_description': 'Free to play game',\n",
    "            'header_image': ''\n",
    "        },\n",
    "        {\n",
    "            'steam_appid': 88888,\n",
    "            'name': 'Very Long Game Name That Might Cause Issues With Text Processing And Database Storage',\n",
    "            'type': 'software',\n",
    "            'is_free': False,\n",
    "            'detailed_description': 'x' * 1000,  # Very long description\n",
    "            'short_description': 'Software application',\n",
    "            'header_image': 'https://example.com/very/long/url/that/might/cause/issues/image.jpg'\n",
    "        }\n",
    "    ])\n",
    "    edge_cases.to_csv(fixtures_dir / 'edge_case_apps.csv', index=False)\n",
    "    \n",
    "    # 3. Price data with edge cases\n",
    "    all_app_ids = minimal_apps['steam_appid'].tolist() + edge_cases['steam_appid'].tolist()\n",
    "    price_data = factory.create_price_dataset(all_app_ids)\n",
    "    \n",
    "    # Add edge cases for prices\n",
    "    price_data[99999] = {'min': 0.0, 'max': 0.0, 'count': 0}  # Free game\n",
    "    price_data[88888] = {'min': 199.99, 'max': 299.99, 'count': 5}  # Expensive software\n",
    "    \n",
    "    with open(fixtures_dir / 'sample_price_minmax.json', 'w') as f:\n",
    "        json.dump({str(k): v for k, v in price_data.items()}, f, indent=2)\n",
    "    \n",
    "    # 4. Sample review data (for review processing tests)\n",
    "    sample_reviews = [\n",
    "        {\n",
    "            'appid': 12345,\n",
    "            'review_text': 'Great game! Highly recommended.',\n",
    "            'voted_up': True,\n",
    "            'votes_helpful': 15,\n",
    "            'votes_funny': 2\n",
    "        },\n",
    "        {\n",
    "            'appid': 12345,\n",
    "            'review_text': 'Not worth the money. Boring gameplay.',\n",
    "            'voted_up': False,\n",
    "            'votes_helpful': 8,\n",
    "            'votes_funny': 0\n",
    "        },\n",
    "        {\n",
    "            'appid': 67890,\n",
    "            'review_text': '',  # Empty review edge case\n",
    "            'voted_up': True,\n",
    "            'votes_helpful': 0,\n",
    "            'votes_funny': 0\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    pd.DataFrame(sample_reviews).to_csv(fixtures_dir / 'sample_reviews.csv', index=False)\n",
    "    \n",
    "    # 5. Configuration fixtures\n",
    "    test_config = {\n",
    "        'embedding_model': 'test-model',\n",
    "        'batch_size': 10,\n",
    "        'max_description_length': 500,\n",
    "        'price_currency': 'USD'\n",
    "    }\n",
    "    \n",
    "    with open(fixtures_dir / 'test_config.json', 'w') as f:\n",
    "        json.dump(test_config, f, indent=2)\n",
    "    \n",
    "    print(f\"Created comprehensive fixtures in {fixtures_dir}\")\n",
    "    \n",
    "    # Show what we created\n",
    "    total_size = 0\n",
    "    for file in fixtures_dir.iterdir():\n",
    "        if file.is_file():\n",
    "            size = file.stat().st_size\n",
    "            total_size += size\n",
    "            print(f\"  {file.name}: {size} bytes\")\n",
    "    \n",
    "    print(f\"\\nTotal fixtures size: {total_size / 1024:.1f} KB\")\n",
    "    \n",
    "    if total_size > 100 * 1024:  # 100 KB limit\n",
    "        print(\"⚠️  WARNING: Fixtures exceed 100 KB size limit!\")\n",
    "    else:\n",
    "        print(\"✅ Fixtures within size guidelines\")\n",
    "\n",
    "# Create the fixtures\n",
    "create_comprehensive_fixtures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34640ae8",
   "metadata": {},
   "source": [
    "## Section 6: Data Mocking Strategies\n",
    "\n",
    "Implement mocking patterns for external data dependencies using tools like pytest fixtures, unittest.mock, or dedicated mocking libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb7df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pytest fixtures for our pipeline tests\n",
    "\n",
    "pytest_fixtures_code = '''\n",
    "# Contents of pipeline/tests/conftest.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unittest.mock import Mock, patch\n",
    "from typing import Dict, Any\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_app_data():\n",
    "    \"\"\"Provide sample app data for tests.\"\"\"\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            'steam_appid': 12345,\n",
    "            'name': 'Test Game 1',\n",
    "            'type': 'game',\n",
    "            'is_free': False,\n",
    "            'detailed_description': 'A detailed description of test game 1',\n",
    "            'short_description': 'Short desc 1'\n",
    "        },\n",
    "        {\n",
    "            'steam_appid': 67890,\n",
    "            'name': 'Test Game 2',\n",
    "            'type': 'game', \n",
    "            'is_free': True,\n",
    "            'detailed_description': 'A detailed description of test game 2',\n",
    "            'short_description': 'Short desc 2'\n",
    "        }\n",
    "    ])\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_price_data():\n",
    "    \"\"\"Provide sample price data for tests.\"\"\"\n",
    "    return {\n",
    "        12345: {'min': 9.99, 'max': 19.99, 'count': 25},\n",
    "        67890: {'min': 0.0, 'max': 0.0, 'count': 1}\n",
    "    }\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_steam_api():\n",
    "    \"\"\"Mock Steam API responses.\"\"\"\n",
    "    with patch('ags_pipeline.extract.steam_client.SteamClient') as mock_client:\n",
    "        mock_instance = Mock()\n",
    "        \n",
    "        # Mock app details response\n",
    "        mock_instance.get_app_details.return_value = {\n",
    "            'success': True,\n",
    "            'data': {\n",
    "                'name': 'Mocked Game',\n",
    "                'type': 'game',\n",
    "                'is_free': False,\n",
    "                'detailed_description': 'Mocked description'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Mock reviews response\n",
    "        mock_instance.get_app_reviews.return_value = {\n",
    "            'success': 1,\n",
    "            'reviews': [\n",
    "                {\n",
    "                    'review': 'Great game!',\n",
    "                    'voted_up': True,\n",
    "                    'votes_helpful': 10\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        mock_client.return_value = mock_instance\n",
    "        yield mock_instance\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_embedder():\n",
    "    \"\"\"Mock embedding model.\"\"\"\n",
    "    with patch('ags_pipeline.embed.nomic_embedder.NomicEmbedder') as mock_embedder:\n",
    "        mock_instance = Mock()\n",
    "        \n",
    "        # Return fixed embeddings for reproducible tests\n",
    "        mock_instance.embed_text.return_value = [0.1, 0.2, 0.3, 0.4, 0.5]  # 5D vector\n",
    "        mock_instance.embed_batch.return_value = [\n",
    "            [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "            [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        ]\n",
    "        \n",
    "        mock_embedder.return_value = mock_instance\n",
    "        yield mock_instance\n",
    "\n",
    "@pytest.fixture\n",
    "def temp_data_dir(tmp_path):\n",
    "    \"\"\"Create temporary data directory with test files.\"\"\"\n",
    "    data_dir = tmp_path / \"data\"\n",
    "    data_dir.mkdir()\n",
    "    \n",
    "    # Create sample CSV\n",
    "    sample_df = pd.DataFrame([\n",
    "        {'steam_appid': 12345, 'name': 'Test Game', 'type': 'game'}\n",
    "    ])\n",
    "    sample_df.to_csv(data_dir / 'expanded_sampled_apps.csv', index=False)\n",
    "    \n",
    "    # Create sample price data\n",
    "    price_data = {'12345': {'min': 9.99, 'max': 19.99, 'count': 25}}\n",
    "    with open(data_dir / 'price_minmax.json', 'w') as f:\n",
    "        json.dump(price_data, f)\n",
    "    \n",
    "    return data_dir\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def mock_external_apis():\n",
    "    \"\"\"Automatically mock all external API calls.\"\"\"\n",
    "    with patch('requests.get') as mock_get:\n",
    "        # Default successful response\n",
    "        mock_response = Mock()\n",
    "        mock_response.status_code = 200\n",
    "        mock_response.json.return_value = {'success': True, 'data': {}}\n",
    "        mock_get.return_value = mock_response\n",
    "        yield mock_get\n",
    "'''\n",
    "\n",
    "print(\"Example pytest fixtures for comprehensive mocking:\")\n",
    "print(pytest_fixtures_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use mocking in actual tests\n",
    "\n",
    "test_example_code = '''\n",
    "# Example test using our mocking strategy\n",
    "def test_etl_with_mocked_data(sample_app_data, sample_price_data, temp_data_dir):\n",
    "    \"\"\"Test ETL pipeline with mocked data.\"\"\"\n",
    "    from ags_pipeline.io.price_minmax_loader import join_price_minmax\n",
    "    \n",
    "    # Use fixture data\n",
    "    result_df = join_price_minmax(sample_app_data, sample_price_data, appid_col='steam_appid')\n",
    "    \n",
    "    # Verify expected columns exist\n",
    "    assert 'price_min' in result_df.columns\n",
    "    assert 'price_max' in result_df.columns\n",
    "    assert 'price_samples_count' in result_df.columns\n",
    "    \n",
    "    # Verify data integrity\n",
    "    assert len(result_df) == len(sample_app_data)\n",
    "    \n",
    "    # Test specific values\n",
    "    test_row = result_df[result_df['steam_appid'] == 12345].iloc[0]\n",
    "    assert test_row['price_min'] == 9.99\n",
    "    assert test_row['price_max'] == 19.99\n",
    "\n",
    "def test_steam_api_integration(mock_steam_api):\n",
    "    \"\"\"Test Steam API integration with mocked responses.\"\"\"\n",
    "    from ags_pipeline.extract.steam_client import SteamClient\n",
    "    \n",
    "    client = SteamClient()\n",
    "    app_data = client.get_app_details(12345)\n",
    "    \n",
    "    # Verify mock was called\n",
    "    mock_steam_api.get_app_details.assert_called_once_with(12345)\n",
    "    \n",
    "    # Verify mocked response\n",
    "    assert app_data['success'] is True\n",
    "    assert app_data['data']['name'] == 'Mocked Game'\n",
    "\n",
    "@pytest.mark.parametrize(\"app_type,expected_free\", [\n",
    "    (\"game\", False),\n",
    "    (\"demo\", True),\n",
    "    (\"dlc\", False)\n",
    "])\n",
    "def test_app_type_logic(app_type, expected_free):\n",
    "    \"\"\"Test business logic with parameterized data.\"\"\"\n",
    "    # Test logic without requiring real data\n",
    "    from ags_pipeline.transform.field_locator import determine_if_free\n",
    "    \n",
    "    result = determine_if_free(app_type, price=0.0)\n",
    "    assert result == expected_free\n",
    "'''\n",
    "\n",
    "print(\"Example test patterns using mocked data:\")\n",
    "print(test_example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87dd90",
   "metadata": {},
   "source": [
    "## Summary: Test Data Management Best Practices\n",
    "\n",
    "Our comprehensive test data strategy for ActualGameSearch V2 includes:\n",
    "\n",
    "### ✅ What We've Implemented\n",
    "\n",
    "1. **Fixture-based Testing**: Small, committed test fixtures (< 10 KB each) with representative data\n",
    "2. **Environment-aware Configuration**: Different data sources for local, CI, and production environments\n",
    "3. **Mock External Dependencies**: Complete mocking of Steam API, embedding models, and other external services\n",
    "4. **Reproducible Data Generation**: Seeded random data generation for consistent test results\n",
    "5. **Edge Case Coverage**: Fixtures include edge cases like empty descriptions, very long names, free games\n",
    "\n",
    "### 🎯 Key Benefits\n",
    "\n",
    "- **Fast CI/CD**: Tests run quickly without downloading large datasets\n",
    "- **Reliable**: No dependency on external APIs or data availability\n",
    "- **Secure**: No sensitive user data in version control\n",
    "- **Maintainable**: Small fixtures are easy to update and understand\n",
    "- **Comprehensive**: Tests cover both happy paths and edge cases\n",
    "\n",
    "### 📁 File Organization\n",
    "\n",
    "```\n",
    "pipeline/\n",
    "├── tests/\n",
    "│   ├── fixtures/              # Small test data files (committed)\n",
    "│   │   ├── sample_expanded_apps.csv\n",
    "│   │   ├── sample_price_minmax.json\n",
    "│   │   ├── edge_case_apps.csv\n",
    "│   │   └── test_config.json\n",
    "│   ├── conftest.py           # Pytest fixtures and mocks\n",
    "│   └── test_*.py            # Test files using fixtures\n",
    "├── data/                    # Real data files (not committed)\n",
    "│   ├── expanded_sampled_apps.csv  # Large datasets\n",
    "│   └── price_minmax.json          # Generated locally\n",
    "└── src/ags_pipeline/        # Source code\n",
    "```\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Expand fixture coverage** as new data types are added\n",
    "2. **Add integration test tier** for testing with larger datasets locally\n",
    "3. **Document data generation scripts** for reproducing test datasets\n",
    "4. **Monitor fixture size** to keep within version control limits\n",
    "5. **Add performance benchmarks** using generated large datasets\n",
    "\n",
    "This approach ensures our tests are fast, reliable, and maintainable while still providing comprehensive coverage of our ETL pipeline functionality."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
